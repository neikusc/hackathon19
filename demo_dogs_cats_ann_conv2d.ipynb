{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Visualizing of confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas  as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DESIRED_ACCURACY = 0.999\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19985 images belonging to 2 classes.\n",
      "Found 5015 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "            directory = '/Users/neikusc/data/dogs_cats/train/',\n",
    "            target_size=(IMG_SIZE,IMG_SIZE),\n",
    "            batch_size = 128,\n",
    "            class_mode = 'binary'\n",
    "        )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "            directory = '/Users/neikusc/data/dogs_cats/valid/',  # This is the source directory for training images\n",
    "            target_size=(IMG_SIZE,IMG_SIZE),         # All images will be resized to IMG_SIZE x IMG_SIZE\n",
    "            batch_size=32,                           # Flow training images in batches of 32 using train_datagen generator\n",
    "            class_mode='binary',                     # Since we use binary_crossentropy loss, we need binary labels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('acc')>DESIRED_ACCURACY):\n",
    "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/neikusc/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/neikusc/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "157/157 [==============================] - 170s 1s/step - loss: 0.5801 - acc: 0.6921\n",
      "157/157 [==============================] - 2003s 13s/step - loss: 0.6939 - acc: 0.6323 - val_loss: 0.5801 - val_acc: 0.6921\n",
      "Epoch 2/5\n",
      "157/157 [==============================] - 178s 1s/step - loss: 0.5176 - acc: 0.7484\n",
      "157/157 [==============================] - 5346s 34s/step - loss: 0.5316 - acc: 0.7366 - val_loss: 0.5176 - val_acc: 0.7484\n",
      "Epoch 3/5\n",
      "157/157 [==============================] - 204s 1s/step - loss: 0.5159 - acc: 0.7623\n",
      "157/157 [==============================] - 1845s 12s/step - loss: 0.4587 - acc: 0.7833 - val_loss: 0.5159 - val_acc: 0.7623\n",
      "Epoch 4/5\n",
      "157/157 [==============================] - 197s 1s/step - loss: 0.4536 - acc: 0.7882\n",
      "157/157 [==============================] - 2077s 13s/step - loss: 0.3945 - acc: 0.8208 - val_loss: 0.4536 - val_acc: 0.7882\n",
      "Epoch 5/5\n",
      "157/157 [==============================] - 1008s 6s/step - loss: 0.4634 - acc: 0.7940\n",
      "157/157 [==============================] - 9440s 60s/step - loss: 0.3186 - acc: 0.8597 - val_loss: 0.4634 - val_acc: 0.7940\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=2,  \n",
    "                epochs=5,\n",
    "                verbose=1,\n",
    "                callbacks=[callbacks],\n",
    "                validation_data = valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots for training and testing process: loss and accuracy\n",
    "plt.figure(0)\n",
    "plt.plot(history.history['acc'],'r')\n",
    "plt.plot(history.history['val_acc'],'g')\n",
    "plt.xticks(np.arange(0, 11, 2.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "plt.legend(['train','validation'])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'],'r')\n",
    "plt.plot(history.history['val_loss'],'g')\n",
    "plt.xticks(np.arange(0, 11, 2.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "# Confusion matrix result\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_pred = model.predict(x_test, verbose=2)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "for ix in range(10):\n",
    "    print(ix, confusion_matrix(np.argmax(y_test,axis=1),y_pred)[ix].sum())\n",
    "cm = confusion_matrix(np.argmax(y_test,axis=1),y_pred)\n",
    "print(cm)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, range(10),\n",
    "                  range(10))\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 12})# font size\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
